---
title: "Lab 6"
author: "Lewis White"
date: "2023-03-01"
output: html_document
---

## Case Study Eel Species Distribution Modeling

This week's lab follows a modeling project described by Elith et al. (2008) (Supplementary Reading)

```{r, warning=FALSE, message=FALSE}
#LOAD PACKAGES
library(tidyverse) #cleaning data, ggplot, etc 
library(here) #for setting working directory
library(tidymodels) #for modeling/statistical analysis
library(rsample) #for splitting data into train / test
library(recipes) #for creating the recipe for ML
library(tictoc) #timing functions/loops
library(xgboost) #gradient boosting 
library(vip) #for determining variable importance
library(pROC) #for calculating roc auc manually for the eval data

```

## Data

Grab the model training data set from the class Git:

data/eel.model.data.csv

```{r}
#READ IN THE DATA 
eels <- read_csv(here("data", "eel.model.data.csv")) %>%
  mutate(Angaus = as.factor(Angaus)) %>%
  select(-Site) #remove site as each site is different so this is essentially an id column
```

### Split and Resample

Split the joined data from above into a training and test set, stratified by outcome score. Use 10-fold CV to resample the training set, stratified by Angaus

```{r}
#split the data
set.seed(123)
eels_split <- initial_split(eels, prop = .7, strata = Angaus) #sample size is just 1000, so I went with a 70% training 30% testing split
eels_train <- training(eels_split)
eels_test <- testing(eels_split)

#setting up resample for future cross validations
set.seed(123)
cv_folds <- eels_train %>%
  vfold_cv(v = 10, strata = Angaus)
```

### Preprocess

Create a recipe to prepare your data for the XGBoost model.  We are interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis

```{r}
#pre-processing
# We need to create a recipe and do the pre-processing by converting dummy coding the nominal variables and normalizing the numeric variables.
eel_recipe <- recipe(Angaus ~ ., data = eels_train) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>% #normalize numeric to make sure scale is okay
  prep()
```

## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined on Monday, first we conduct tuning on just the learn_rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()


```{r}
#model specification just tuning the learning rate
eel_spec_lr_tune <- boost_tree(trees = 3000, #starting with large number of trees (e.g. 3000 is recommended)
                               learn_rate = tune()) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

#create a workflow
eel_lr_tune_wf <- workflow() %>%
  add_recipe(eel_recipe) %>%
  add_model(eel_spec_lr_tune)
```

2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run. You could use {tictoc} or Sys.time().

```{r}
#creating a gird of learning rate values to tune so we can find optimal value
learn_rate_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

tic() #start timer

#set up code to run using parallel processing
doParallel::registerDoParallel() 

set.seed(123)

#tuning the learn rate
boost_rs <- tune_grid(
  eel_lr_tune_wf,
  Angaus ~ ., #function 
  resamples = cv_folds, #resamples to use
  grid = learn_rate_grid, #grid to try
  metrics = metric_set(accuracy, roc_auc, pr_auc) #how to assess which combinations are best 
)

toc() #end timer

```


3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}
#showing the best options for the learn_rate value
show_best(boost_rs, metric = "roc_auc")
```

### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

```{r}
# XGBoost model specification
eel_tree_param_spec <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 3000,
    min_n = tune(),
    tree_depth = tune(),
    loss_reduction = tune(),
    learn_rate = select_best(boost_rs, metric = "roc_auc")$learn_rate) %>%
    set_engine("xgboost")

```


2.  Set up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space

```{r}
# grid specification
xgboost_tree_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    loss_reduction())


#grid_max_entropy:  construct parameter grids that try to cover the parameter space such that any portion of the space has an observed combination that is not too far from it.
xgboost_tree_params_grid <- 
  dials::grid_max_entropy( 
    xgboost_tree_params,  
    size = 50 #number of different parameter combinations 
  )

xgboost_tree_params_wf <- 
  workflows::workflow() %>%
  add_model(eel_tree_param_spec) %>% 
  add_recipe(eel_recipe)

tic()

set.seed(123)

# hyperparameter tuning
xgboost_tree_params_tuned <- tune::tune_grid(
  object = xgboost_tree_params_wf,
  resamples = cv_folds,
  grid = xgboost_tree_params_grid,
  metrics = yardstick::metric_set(accuracy, roc_auc, pr_auc),
  #control = tune::control_grid(verbose = TRUE)
)

toc()
```

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
#show the performance of the best models based on the roc area under the curve metric 
show_best(xgboost_tree_params_tuned, metric = "roc_auc")
```


### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

```{r}
#goal: tune stochastic parameters mtry and sample size 

# XGBoost model specification
eel_stochastic_spec <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 3000, #number of trees contained in the ensemble
    min_n = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$min_n, #minimum number of data points in a node that is required for node to be split further
    tree_depth = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$tree_depth, #maximum depth of tree (i.e. number of splits)
    learn_rate = select_best(boost_rs, metric = "roc_auc")$learn_rate, #the rate at which the bosting algorithm adapts from iteration-to-iteration
    loss_reduction = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$loss_reduction, #the reduction in the loss function required to split further
    mtry = tune(), #number predictors randomly sampled at each split
    sample_size = tune(), #the amount of data exposed to the fitting routine
    stop_iter = tune()) %>% #the number of iterations without improvement before stopping 
    set_engine("xgboost")
```


2.  Set up a tuning grid. Use grid_max_entropy() again.

```{r}
# grid specification
xgboost_stochastic_params <- 
  dials::parameters(finalize(mtry(), select(eels_train, -Angaus)), #mtry values will range from 1 to the number of predictors included in the model
                    sample_size = sample_prop(c(.4, .9)), #sample between 40% and 90% of observations 
                    stop_iter()) #The number of iterations without improvement before stopping

xgboost_stochastic_grid <- 
  dials::grid_max_entropy(
    xgboost_stochastic_params, 
    size = 50
  )

#create workflow
xgboost_stochastic_wf <- 
  workflows::workflow() %>%
  add_model(eel_stochastic_spec) %>% 
  add_recipe(eel_recipe)

tic()

set.seed(123)

# hyperparameter tuning
xgboost_stochastic_tuned <- tune::tune_grid(
  object = xgboost_stochastic_wf,
  resamples = cv_folds,
  grid = xgboost_stochastic_grid,
  metrics = yardstick::metric_set(accuracy, roc_auc, pr_auc),
  #control = tune::control_grid(verbose = TRUE)
)

toc()
```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
show_best(xgboost_stochastic_tuned, metric = "roc_auc")

 

#This code would create graphs for the mean log loss results for various values of mtry, sample size, and stop_iter

#library(data.table) 

# xgboost_stochastic_tuned %>% collect_metrics() %>%
#   select(mean, mtry, sample_size, stop_iter) %>%
#   data.table() %>%
#   melt(id = "mean") %>%
#   ggplot(aes(y=mean,x=value,colour=variable)) +
#   geom_point(show.legend = FALSE) +
#   facet_wrap(variable~. , scales="free") + theme_bw() +
#   labs(y="Mean log-loss", x = "Parameter")
```


## Finalize workflow and make final prediction

1.  Assemble your final workflow will all of your optimized parameters and do a final fit.

```{r}
#create the final model using all of the optimal tuned parameter values

full_model_spec <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 3000, #number of trees contained in the ensemble
    min_n = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$min_n, #minimum number of data points in a node that is required for node to be split further
    tree_depth = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$tree_depth, #maximum depth of tree (i.e. number of splits)
    learn_rate = select_best(boost_rs, metric = "roc_auc")$learn_rate, #the rate at which the bosting algorithm adapts from iteration-to-iteration
    mtry = select_best(xgboost_stochastic_tuned, metric = "roc_auc")$mtry, #number predictors randomly sampled at each split
    loss_reduction = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$loss_reduction, #the reduction in the loss function required to split further
    sample_size = select_best(xgboost_stochastic_tuned, metric = "roc_auc")$sample_size, #the amount of data exposed to the fitting routine
    stop_iter = select_best(xgboost_stochastic_tuned, metric = "roc_auc")$stop_iter) %>% #the number of iterations without improvement before stopping 
    set_engine("xgboost")
```

```{r}
#final workflow
final_wf <- workflow() %>%
  add_recipe(eel_recipe) %>%
  add_model(full_model_spec)

set.seed(123)
#pop my spec into a workflow for final fit 
final_fit <- last_fit(final_wf, eels_split)

final_fit %>% collect_metrics()

test_eel_preds <- final_fit$.predictions[[1]]

#create confusion matrix 
conf_matrix <- test_eel_preds %>% yardstick::conf_mat(truth=Angaus, estimate=.pred_class) 

autoplot(conf_matrix, type = "heatmap") + 
  labs(title = "Confusion Matrix for Test Data")
```

2. How well did your model perform? What types of errors did it make?

**My model performed alright, but could certainly have performed better. The accuracy was 0.827 with an roc_auc of 0.818. Given that almost 80% of the sample sites did not have the Angaus species, my model is not much better than guessing no specied detection for each site.**

**The model appears to over-predict false negative results. The false positive rate is 6.7% while the false negative rate is 59%. This is likely due to the class imbalance (many more sites did not have the Angaus species of eel detected).**


## Fit your model the evaluation data and compare performance

1.  Now fit your final model to the big dataset: data/eval.data.csv

```{r}
#READ IN THE DATA 
eels_eval <- read_csv(here("eel.eval.data.csv")) %>%
  rename(Angaus = Angaus_obs) %>%
  mutate(Angaus = as.factor(Angaus))
```

2.  How does your model perform on this data?

```{r}
#fit the final model on the evaluation data 
set.seed(123)
eels_eval_fit <- fit(final_wf, data = eels_eval) 

eel_eval_preds <- predict(eels_eval_fit, new_data = eels_eval)

eels_eval_set <- bind_cols(eels_eval$Angaus, eel_eval_preds) %>%
  rename(Angaus = ...1)

#make the confusion matrix 
conf_matrix <- eels_eval_set %>% yardstick::conf_mat(truth=Angaus, estimate=.pred_class) 

autoplot(conf_matrix, type = "heatmap") + 
  labs(title = "Confusion Matrix for Evaluation Data")
```



3.  How do your results compare to those of Elith et al.?

```{r}
#Calculating the roc auc so I can compare to the paper 

#obtaining true values and making them numeric 
true_labels <- as.numeric(eels_eval_set$Angaus) - 1 

#make the predictions numeric as well
predictions <- as.numeric(eels_eval_set$.pred_class) - 1

#create roc object from pROC package
roc_obj <- roc(true_labels, predictions)

#calculate roc auc 
auc(roc_obj)
```

**The roc auc I obtained was slightly worse than the 0.858 obtained in the paper. It's possible that adjusting my grids to allow for more options (e.g. more trees, different learning rates) could help improve the auc here.**

-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?

```{r}
#figuring out the variable importance 

#bake the new data because vip isn't working for workflows
baked_eels_eval <- eel_recipe %>%
  bake(eels_eval)

set.seed(123)
full_model_spec %>%
  fit(Angaus~., data = baked_eels_eval) %>%
  vip(geom = "col", #column chart
      num_features = 12, #all 12 features 
      mapping = aes_string(fill = "Importance")) + #color determined by importance 
  theme_minimal() +
  labs(title = "Chart of Variable Importance")

#determining what effect the most important variables had on the result
eels_eval %>%
  filter(Angaus == 1) %>%
  summarize(ocean_temp = mean(SegSumT),
            distance_coast = mean(DSDist))

eels_eval %>%
  filter(Angaus == 0) %>%
  summarize(ocean_temp = mean(SegSumT),
            distance_coast = mean(DSDist))
```

**Summer air temperature (SegSumT) and distance to coast (DSDist) were the most important predictors in this model. In general, sites where the Angaus eel was detected were more likely to be slightly warmer and closer to the coast than areas where the eel was not detected.**

**On the other end of the spectrum, DSDam (Presence of known downstream obstructions, mostly dams) was particularly unimportant in the prediction process.**


```{r, include=FALSE, eval=FALSE}
#Initial way of learning abotu test data 

#baked test data 
baked_eels_test <- eel_recipe %>% bake(eels_test)

set.seed(123)
#fit final model on the test data 
eels_test_fit <- full_model_spec %>% fit(Angaus~., data = baked_eels_test)

#make predictions for the test data
eels_test_preds <- eels_test_fit %>% 
  predict(new_data = baked_eels_test)

#find associated probabilities for the predictions 
eels_test_pred_probs <- eels_test_fit %>% 
  predict_classprob.model_fit(new_data = baked_eels_test)

# prepare data frame with true value and prediction for confusion matrix 
eels_test_set <- bind_cols(baked_eels_test$Angaus, eels_test_preds, eels_test_pred_probs) %>%
  rename(Angaus = ...1)

#make confusion matrix 
conf_matrix <- eels_test_set %>% yardstick::conf_mat(truth=Angaus, estimate=.pred_class) 

autoplot(conf_matrix, type = "heatmap") + 
  labs(title = "Confusion Matrix for Test Data")





#Initial method for checking eval data

#bake the new data
baked_eels_eval <- eel_recipe %>%
  bake(eels_eval)

#fit the final model on the evaluation data 
set.seed(123)
eels_eval_fit <- full_model_spec %>% fit(Angaus~., data = baked_eels_eval)
eels_eval_fit

#make predictions for the test data
eels_eval_preds <- eels_eval_fit %>% 
  predict(new_data = baked_eels_eval)

#find associated probabilities with the predictions
eels_eval_pred_probs <- eels_eval_fit %>% 
  predict_classprob.model_fit(new_data = baked_eels_eval)

#create df for confusion matrix 
eels_eval_set <- bind_cols(baked_eels_eval$Angaus, eels_eval_preds, eels_eval_pred_probs) %>%
  rename(Angaus = ...1)

#make the confusion matrix 
conf_matrix <- eels_eval_set %>% yardstick::conf_mat(truth=Angaus, estimate=.pred_class) 

autoplot(conf_matrix, type = "heatmap") + 
  labs(title = "Confusion Matrix for Evaluation Data")

```


