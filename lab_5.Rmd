---
title: "Lab5"
author: "Lewis White"
date: "2023-02-07"
output: html_document
---

This week's lab is a musical lab. You'll be requesting data from the Spotify API and using it to build k-nearest neighbor and decision tree models. In order to use the Spotify you must have a Spotify account. If you don't have one, sign up for a free one here: <https://www.spotify.com/us/signup>.

Once you have an account, go to Spotify for developers (<https://developer.spotify.com/>) and log in. Click the green "Create a Client ID" button to fill out the form to create an app create an app so you can access the API. On your developer dashboard page, click on the new app you just created. On the app's dashboard page you will find your Client ID just under the header name of your app. Click "Show Client Secret" to access your secondary Client ID. When you do this you'll be issued a Spotify client ID and client secret key.

You have two options for completing this lab.

**Option 1**: **Classify by users**. Build models that predict whether a given song will be in your collection vs. a partner in class. This requires that you were already a Spotify user so you have enough data to work with. You will download your data from the Spotify API and then exchange with another member of class.

-   **I am selecting this option.**

**Option 2**: **Classify by genres**. Build models that predict which genre a song belongs to. This will use a pre-existing Spotify dataset available from Kaggle.com (<https://www.kaggle.com/datasets/mrmorj/dataset-of-songs-in-spotify>)

```{r}
#LOAD NECESSARY PACKAGES

library(spotifyr) #API interaction
library(tidyverse) #cleaning data, ggplot, etc 
library(tidymodels) #for modeling/statistical analysis
library(rsample) #for splitting data into train / test
library(recipes) #for creating the recipe for ML
#library(skimr) #for data exploration / early summary stats and viz
library(kknn) #for KNN modeling
library(plotly) #for data viz
library(ggpubr) #for data viz
library(here) #for simplifying file path navigation
```

Client ID and Client Secret are required to create and access token that is required to interact with the API. You can set them as system values so we don't have to do provide them each time.

```{r, eval = FALSE}
#API key stored in token.R, which is not git tracked for security

source(here("token.R"))
```

This may result in an error: "INVALID_CLIENT: Invalid redirect URI." This can be resolved by editing the callback settings on your app. Go to your app and click "Edit Settings". Under redirect URLs paste this: <http://localhost:1410/> and click save at the bottom.

### **Option 1: Data Preparation**

You can use get_my_saved_tracks() to request all your liked tracks. It would be good if you had at least 150-200 liked tracks so the model has enough data to work with. If you don't have enough liked tracks, you can instead use get_my_recently_played(), and in that case grab at least 500 recently played tracks if you can.

The Spotify API returns a dataframe of tracks and associated attributes. However, it will only return up to 50 (or 20) tracks at a time, so you will have to make multiple requests. Use a function to combine all your requests in one call.

```{r}
#GETTING A DATAFRAME OF LIKED SONGS

#In the get_my_saved_tracks() function, you can use offsets() to specify the index of the first track to return. 
offsets = seq(from = 0, to = 1000, by = 50)

#initializing an empty matrix 
liked_tracks <- data.frame(matrix(nrow = 0, ncol = 30))

#Function to get my 1050 most recently liked tracks 
for (i in seq_along(offsets)) {  
  my_tracks = get_my_saved_tracks(limit = 50, 
                                     offset = offsets[i])
  df_temp = as.data.frame(my_tracks) #creating a temporary data frame to store the 50 liked tracks from a given run
  liked_tracks <- rbind(liked_tracks, df_temp) #binding the temporary data frame to my liked tracks data frame. 
}
```

Once you have your tracks, familiarize yourself with this initial dataframe. You'll need to request some additional information for the analysis. If you give the API a list of track IDs using get_track_audio_features(), it will return an audio features dataframe of all the tracks and some attributes of them.

```{r}
#obtain a list of the track IDs 
ids <- liked_tracks$track.id

#the ids argument in `get_track_audio_features()` can only take 100 IDs at a time, so I'm splitting them into 100 track groupings
ids_split <- split(ids, ceiling(seq_along(ids) / 100))

#create an empty list
my_tracks_audio_feats_list <- list()

#Iterating the `get_track_audio_features()`function over each ID split and storing the resulting data in a list.
for (i in 1:length(ids_split)) {
  my_tracks_audio_feats_list[[i]] <- get_track_audio_features(ids = ids_split[[i]])
}

#Combine the list of data frames into a single data frame.
my_tracks_audio_feats <- do.call(rbind, my_tracks_audio_feats_list)
```

These track audio features are the predictors we are interested in, but this dataframe doesn't have the actual names of the tracks. Append the 'track.name' column from your favorite tracks database.

```{r}
#selecting the track id and track name from liked tracks so I can use left_join and only add the track name to the audio features data set
liked_tracks_join <- liked_tracks %>%
  select(track.id, track.name, track.artists) %>%
  mutate(primary_artist = unlist(lapply(liked_tracks$track.artists, function(x) x$name[1]))) %>%
  select(-track.artists)



#add the track name and artist to the audio features data frame 
liked_tracks_full <- left_join(my_tracks_audio_feats, liked_tracks_join, by = c("id" = "track.id"))

#save this data frame as a CVS so it can be shared with others
write_csv(liked_tracks_full, "lewis_liked_tracks.csv")
```

Find a class mate whose data you would like to use. Add your partner's data to your dataset. Create a new column that will contain the outcome variable that you will try to predict. This variable should contain two values that represent if the track came from your data set or your partner's.

```{r}
# reading in Elke's liked tracks. Add a column to make it clear which tracks are Elke's  
elke_liked_tracks <- read_csv(here("elke_liked_tracks.csv")) %>%
  mutate(listener = "Elke")

#add a column named "listener" to make it clear that these are my tracks 
lewis_liked_tracks <- liked_tracks_full %>%
  mutate(listener = "Lewis")

# Combine all of our liked tracks into one data frame 
all_tracks <- rbind(lewis_liked_tracks, elke_liked_tracks) %>%
  select(-(type:analysis_url))
```

### Data Exploration 

Let's take a look at your data. Do some exploratory summary stats and visualization.

For example: What are the most danceable tracks in your dataset? What are some differences in the data between users (Option 1) or genres (Option 2)?

```{r}
# names(lewis_liked_tracks)  #for checking out column names
lewis_liked_tracks %>%
  select(danceability, track.name, primary_artist) %>%
  arrange(-danceability) %>%
  head(5)
  
lewis_liked_tracks %>%
  select(danceability, track.name, primary_artist) %>%
  arrange(danceability) %>%
  head(5)

lewis_liked_tracks %>%
  select(speechiness, track.name, primary_artist) %>%
  arrange(-speechiness) %>%
  head(5)
  
lewis_liked_tracks %>%
  select(speechiness, track.name, primary_artist) %>%
  arrange(speechiness) %>%
  head(5)
```

#### Distribution Plots

```{r}
#Danceability comparison
dance_plot <- ggplot(all_tracks, aes(x = danceability, fill = listener,
                    text = paste(listener))) +
  geom_density(alpha=0.6, color=NA) +
  scale_fill_manual(values=c("#b0484f", "#4436d9"))+
  labs(x="Danceability", y="Density") +
  guides(fill=guide_legend(title="Listener"))+
  theme_minimal() +
  ggtitle("Distribution of Danceability Data")


#speechiness comparison
speech_plot <- ggplot(all_tracks, aes(x = speechiness, fill = listener,
                    text = paste(listener))) +
  geom_density(alpha=0.6, color=NA) +
  scale_fill_manual(values=c("#b0484f", "#4436d9"))+
  labs(x="Speechiness", y="Density") +
  guides(fill=guide_legend(title="Listener"))+
  theme_minimal() +
  ggtitle("Distribution of Speechiness Data")


#acousticness comparison
acoustic_plot <- ggplot(all_tracks, aes(x = acousticness, fill = listener,
                    text = paste(listener))) +
  geom_density(alpha=0.6, color=NA) +
  scale_fill_manual(values=c("#b0484f", "#4436d9"))+
  labs(x="Acousticness", y="Density") +
  guides(fill=guide_legend(title="Listener"))+
  theme_minimal() +
  ggtitle("Distribution of Acousticness Data")

energy_plot <- ggplot(all_tracks, aes(x = energy, fill = listener,
                    text = paste(listener))) +
  geom_density(alpha=0.6, color=NA) +
  scale_fill_manual(values=c("#b0484f", "#4436d9"))+
  labs(x="Energy", y="Density") +
  guides(fill=guide_legend(title="Listener"))+
  theme_minimal() +
  ggtitle("Distribution of Energy Data")

ggarrange(dance_plot, speech_plot, acoustic_plot, energy_plot, ncol=2, nrow=2, common.legend = TRUE, legend="bottom")
```

```{r}
ggplot(data = all_tracks, aes(x = valence, y = energy, color = listener)) +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = 0.5) +
  geom_hline(yintercept = 0.5) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +
  annotate('text', 0.25 / 2, 0.95, label = "Turbulent/Angry", fontface =
             "bold") +
  annotate('text', 1.75 / 2, 0.95, label = "Happy/Joyful", fontface = "bold") +
  annotate('text', 1.75 / 2, 0.05, label = "Chill/Peaceful", fontface =
             "bold") +
  annotate('text', 0.25 / 2, 0.05, label = "Sad/Depressing", fontface =
             "bold") +
  theme_minimal()


## PLOTLY SHOWING VALENCE
plot_ly(data = all_tracks,
        x = ~valence, y = ~energy, color = ~listener, type = "scatter", mode = "markers",
        text = paste("Track Name:", all_tracks$track.name, "<br>",
                     "Primary Artist:", all_tracks$primary_artist, "<br>",
                     "Valence:", all_tracks$valence, "<br>", 
                     "Energy:", all_tracks$energy, "<br>",
                     "Listener:", all_tracks$listener)) %>%
  layout(xaxis = list(title = "Valence"),
         yaxis = list(title = "Energy"),
         hovermode = "closest",
         title = "Track Valence vs Energy")
```

### **Modeling**

Create two models, a k-nearest neighbor model and a decision tree model that predict whether a track belongs to you or your partner's collection. Then validate and compare the performance of the two models you have created.

Make sure to use appropriate resampling to select the best version of each algorithm to compare and some appropriate visualization of your results.

#### KNN approach

```{r}
all_tracks_knn <- all_tracks %>% mutate_if(is.ordered, .funs = factor, ordered = F) %>%
  select(-track.name, -primary_artist)
# We need to create a recipe and do the pre-processing by converting dummy coding the nominal variables and normalizing the numeric variables.


#pre-processing
knn_rec <- recipe(listener ~ ., data = all_tracks_knn) %>%
  #step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) We have no categorical variables that we want to encode as a dummy variable 
  step_normalize(all_numeric(), -all_outcomes()) %>% #normalize numeric to make sure scale is okay
  prep()

#bake 
baked_tracks <- bake(knn_rec, all_tracks_knn)

```

#### splitting the data

```{r}
set.seed(123)
#initial split of data ~ we're going with 70/30 because the sample size isn't super large for testing
tracks_split <- initial_split(baked_tracks, prop = .7)
tracks_train <- training(tracks_split)
tracks_test <- testing(tracks_split)
```

### Tuning the model

```{r}
set.seed(123)
# 10-fold CV on the training dataset
cv_folds <- tracks_train %>%
  vfold_cv(v = 10)

cv_folds
```

```{r spec_with_tuning}
# Define our KNN model with tuning
knn_spec_tune <- nearest_neighbor(neighbor = tune()) %>%
set_mode("classification") %>%
set_engine("kknn")

# Check the model
knn_spec_tune
```

```{r}
# Define a new workflow
wf_knn_tune <- workflow() %>%
add_model(knn_spec_tune) %>%
add_recipe(knn_rec)
    
# Fit the workflow on our predefined folds and hyperparameters
fit_knn_cv <- wf_knn_tune %>%
  tune_grid(
    cv_folds, 
    grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))) # try with 1 nearest neighbor, try with 5, 10, 20, 30, ..., 100
  )
    
# Check the performance with collect_metrics()
print(n=24, fit_knn_cv %>% collect_metrics()) 
```

```{r}
# The final workflow for our KNN model
final_wf <-
  wf_knn_tune %>%
  finalize_workflow(select_best(fit_knn_cv))

# Check out the final workflow object
final_wf
```

```{r}
# Fitting our final workflow
final_fit <- final_wf %>%
  fit(data = tracks_train)

# Examine the final workflow
final_fit
```

### Apply model to the test data

```{r}
#generating predictions using the model on the test data
tracks_pred <- final_fit %>% predict(new_data = tracks_test)

tracks_pred %>% head()

```

```{r}
# Write over 'final_fit' with this last_fit() approach
final_fit <- final_wf %>% last_fit(tracks_split)

# Collect metrics on the test data!
final_fit %>% collect_metrics()
```

### Decision Tree Approach


```{r}
#new spec, tell the model that we are tuning hyperparams
tree_spec_tune <- decision_tree(
  cost_complexity = tune(),   #tune() asks R to try a nunch of different parameters. 
  tree_depth = tune(),
  min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")


tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5) #grid_regular shows grid of different input tuning options
#levels says how many combinations we should try. 

tree_grid  #125 options 
```

```{r}
wf_tree_tune <- workflow() %>%
  add_recipe(knn_rec) %>%  #when you add recipe into workflow, it automatically will prep and bake when necessary. 
  add_model(tree_spec_tune)

#workflow pulls together the specification. and then we can fit on the workflow. you could fit the wf_tree_tune 
```

```{r}
#set up k-fold cv. This can be used for all the algorithms

genre_cv = genre_train %>% vfold_cv(v=5) #creating 5 folds cross validation 
genre_cv
```

```{r}
doParallel::registerDoParallel() #build trees in parallel
#200s

tree_rs <- tune_grid(
  tree_spec_tune, #model specification 
  genre ~ ., #function 
  resamples = genre_cv, #resamples to use
  grid = tree_grid, #grid to try
  metrics = metric_set(accuracy) #how to assess which combinations are best 
)

tree_rs
```

```{r}
autoplot(tree_rs) + theme_light()
```

```{r select_hyperparam}
show_best(tree_rs)
select_best(tree_rs)
```



### Decision Tree Approach

```{r}
#load packages specific to decision trees 

# Modeling packages
library(rpart)       # direct engine for decision tree application
library(caret)       # meta engine for decision tree application

# Model interpretability packages
library(rpart.plot)  # for plotting decision trees
library(vip)         # for feature importance
library(pdp)         # for feature effects
```

```{r}
#creating an initial decision tree
music_dt <- rpart(
  formula = listener ~ .,
  data    = tracks_train,
  method  = "class"
)

#plotting the full decision tree
rpart.plot(music_dt, main = "Predicting Listeners: A Decision Tree Approach") 

#plotting the relative cross validation error (y axis) by various values of cp (a hyper-parameter than penalizes models with more nodes). This also shows the number of nodes at various values of cp. 
plotcp(music_dt)
```

```{r}
#repeating the model above but using a tree that will grow until every terminal node only contains one class
music_dt2 <- rpart(
  formula = listener ~ .,
  data    = tracks_train,
  method  = "class",
  control = list(cp = 0, xval = 10)  #cp is the "complexity parameter" and is used as a threshold to determine when to stop splitting a tree. In this case, cp = 0 means that the tree will grow until every terminal node only contains one class.
)

#plotting the cp vs error graph again for the new model
plotcp(music_dt2)
```

```{r}
# caret cross validation results so we can see RMSE
music_dt3 <- train(
  listener ~ .,
  data = tracks_train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 20
)

music_dt3

ggplot(music_dt3) + 
  theme_minimal() + 
  labs(title = "Accuracy of model by value of cp")

#according to the table of results and associated graph, it looks like a penalty term value of 0.01417563 maximizes the model accuracy
```

```{r}
#creating a final model with the optimal value of cp used.
optimal_model <- rpart(
  formula = listener ~ .,
  data    = tracks_train,
  method  = "class",
  control = list(cp = 0.014175632, xval = 10)
)
```

```{r}
#making a chart that shows the most important features in my model 
vip(optimal_model, num_features = 14, bar = FALSE) +
  theme_minimal() +
  labs(title = "Most important predictors in the model")
```

```{r}
#test model on training data

predictions_train <- predict(optimal_model, data = tracks_train, type = "class")
predictions_train

#creating a confusion matrix to view results
confusionMatrix(
  data = factor(predictions_train),
  reference = factor(tracks_train$listener)
)

```

### Testing my decision tree model on the test data

```{r}
#using the optimal model to make predictions on the test data
predictions <- predict(optimal_model, newdata = tracks_test, type = "class")
predictions

#creating a confusion matrix to view results
confusionMatrix(
  data = factor(predictions),
  reference = factor(tracks_test$listener)
)


#             Reference
# Prediction  Elke Lewis
#      Elke    59    50
#      Lewis   79   277
#                                           
# Accuracy : 0.7226  

#The model is not great considering that guessing Lewis as the listener for each song would result in a 70% accuracy due to the class imbalance. Additionally, the accuracy when training was 0.79. I think it's possible that we overfit the model and a higher cp should have been used. 
```


#Now with an added random forest component and some clarification your task.


## Starting with bagged decision trees

```{r}
bag_tree(mode = "classification",
         cost_complexity = 0,
         tree_depth = NULL,
         min_n = 2,
         class_cost = NULL,
         engine = "rpart")
```




Create four final candidate models:

1. k-nearest neighbor 
2. decision tree
3. bagged tree
    - bag_tree()
    - Use the "times =" argument when setting the engine during model specification to specify the number of trees. The rule of thumb is that 50-500 trees is usually sufficient.  The bottom of that range should be sufficient here.  
    
4. random forest
    - rand_forest()
    - m_try() is the new hyperparameter of interest for this type of model.  Make sure to include it in your tuning process

Go through the modeling process for each model: 
* Preprocessing. You can use the same recipe for all the models you create. 
* Resampling. Make sure to use appropriate resampling to select the best version created by each algorithm.
* Tuning. Find the best values for each hyperparameter (within a reasonable range).

Compare the performance of the four final models you have created.  

Use appropriate performance evaluation metric(s) for this classification task. A table would be a good way to display your comparison.  Use at least one visualization illustrating your model results.