---
title: "Lab5_Demo"
author: "Lewis White"
date: "2023-02-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)    
library(ggplot2)   
library(rsample)   
library(recipes)
library(skimr)
library(tidymodels)
library(kknn)
```

###k-nearest neighbor in tidymodels

## Data

```{r data}
data(attrition)
churn <- attrition %>% mutate_if(is.ordered, .funs = factor, ordered = F) 
#skim(churn_dat)
```

Not doing the data exploration here in the interest of time and since we are familiar with this dataset.

```{r initial_split}
set.seed(123)

#initial split of data, default 75/25

churn_split <- initial_split(churn)

churn_train <- training(churn_split)
churn_test <- testing(churn_split)
```


We need to create a recipe and do the preprocessing by converting dummy coding the nominal variables and normalizing the numeric variables.

```{r recipe}
#preprocessing
knn_rec <- recipe(Attrition ~ ., data = churn_train) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% #encode categorical variables
  step_normalize(all_numeric(), -all_outcomes()) %>% #normalize numeric to make sure scale is okay
  prep()

#bake 
baked_churn <- bake(knn_rec, churn_train)
```

Recall: if you want to explore the what the recipe is doing to your data, you can first prep() the recipe to estimate the parameters needed for each step and then bake(new_data = NULL) to pull out the training data with those steps applied.

Now the recipe is ready to be applied to the test data.

```{r bake_test}
baked_test <- bake(knn_rec, churn_test)
```


##Specify the k-nearest neighbor model

```{r knn_spec}
knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")
```

```{r}
knn_fit <- knn_spec %>% 
  fit(Attrition ~. , data = churn_train)

knn_fit # k = 5 is the default. best to use method to determine optimal k. 
```


```{r cv}
set.seed(123)
# 10-fold CV on the training dataset
cv_folds <- churn_train %>%
  vfold_cv(v = 5)

cv_folds
```

We now have a recipe for processing the data, a model specification, and CV splits for the training data.

Let's put it all together in a workflow.

```{r}
knn_workflow <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)
```

Now fit the resamples.
```{r}
knn_res <- knn_workflow %>%
  fit_resamples(
    resamples = cv_folds,
    control = control_resamples(save_pred = TRUE)
  )

# Check the performance
knn_res %>%
  collect_metrics()
```



```{r spec_with_tuning}
# Define our KNN model with tuning
knn_spec_tune <- nearest_neighbor(neighbor = tune()) %>%
set_mode("classification") %>%
set_engine("kknn")

# Check the model
knn_spec_tune
```


```{r}
# Define a new workflow
wf_knn_tune <- workflow() %>%
add_model(knn_spec_tune) %>%
add_recipe(knn_rec)
    
# Fit the workflow on our predefined folds and hyperparameters
fit_knn_cv <- wf_knn_tune %>%
  tune_grid(
    cv_folds, 
    grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))) # try with 1 nearest neighbor, try with 5, 10, 20, 30, ..., 100
  )


    
# Check the performance with collect_metrics()
fit_knn_cv %>% collect_metrics()
```

This time before we fit the model we need to tell R which values to try for the parameter that we're tuning.

To tune our hyperparameter(s), we will use the tune_grid() function (instead of the fit() or fit_resamples() functions).

This tune_grid() is similar to fit_resamples() except that it takes an additional argument: grid. We will pass the possible values of our hyperparameter(s) to this grid argument, and it will evaluate each fold of our sample on each set of hyperparameters passed to grid.

And finally, we will predict.

The finalize_workflow() function wants (1) your initial workflow and (2) your best model.

```{r}
# The final workflow for our KNN model
final_wf <-
  knn_workflow %>%
  finalize_workflow(select_best(fit_knn_cv))

# Check out the final workflow object
final_wf
```

```{r}
# Fitting our final workflow
final_fit <- final_wf %>%
  fit(data = churn_train)

# Examine the final workflow
final_fit
```

And finally, we can predict onto the testing dataset.

```{r}
churn_pred <- final_fit %>% predict(new_data = churn_test)

churn_pred %>% head()
```

There's a better way! You can pass your final workflow (workflow plus the best model) to the last_fit() function along with your initial split (for us: churn_split) to both (a) fit your final model on your full training dataset and (b) make predictions onto the testing dataset (defined in your initial split object).

This last_fit() approach streamlines your work (combining steps) and also lets you easily collect metrics using the collect_metrics() function

```{r}
# Write over 'final_fit' with this last_fit() approach
final_fit <- final_wf %>% last_fit(churn_split)

# Collect metrics on the test data!
final_fit %>% collect_metrics()
```





## KAGGLE DATA CODE WALKTHROUGH



```{r}
kaggle_dat <- read_csv(here("genres_v2.csv"))
unique(kaggle_dat$genre)
table(kaggle_dat$genre)


#Removing inappropriate columns and selecting trap and Hiphop as the two genres here and making case consistent
genre_dat <- kaggle_dat %>%
  select(-c(type, uri, track_href, analysis_url, `Unnamed: 0`, title, tempo, id, song_name)) %>%
  filter(genre == "Hiphop"|genre == "Rap") %>%
  mutate(genre = str_replace(genre, "Hiphop", "hiphop")) %>%
  mutate(genre = str_replace(genre, "Rap", "rap")) %>%
  mutate(genre = as.factor(genre))
```

```{r}
##split the data
genre_split <- initial_split(genre_dat)
genre_train <- training(genre_split)
genre_test <- testing(genre_split)
```

```{r recipe}
#Preprocess the data

genre_rec <- recipe(genre ~ ., data = genre_train) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  step_normalize(all_numeric(), -all_outcomes())
```

Set up a decision tree specification. Note: the cost_complexity parameter is a pruning penalty parameter that controls how much we penalize the number of terminal nodes in the tree.  It's conceptually similar to lambda from regularized regression.

```{r tree_specification}
tree_spec_fixed <- decision_tree(
  cost_complexity = 0.1, 
  tree_depth = 4,
  min_n = 11
) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

But, as usual, we don't want just any old values for our hyperparameters, we want optimal values.
```{r}
#new spec, tell the model that we are tuning hyperparams
tree_spec_tune <- decision_tree(
  cost_complexity = tune(),   #tune() asks R to try a nunch of different parameters. 
  tree_depth = tune(),
  min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")


tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5) #grid_regular shows grid of different input tuning options
#levels says how many combinations we should try. 

tree_grid  #125 options 
```

```{r workflow_tree}
wf_tree_tune <- workflow() %>%
  add_recipe(genre_rec) %>%  #when you add recipe into workflow, it automatically will prep and bake when necessary. 
  add_model(tree_spec_tune)

#workflow pulls together the specification. and then we can fit on the workflow. you could fit the wf_tree_tune 
```

```{r resampling}
#set up k-fold cv. This can be used for all the algorithms

genre_cv = genre_train %>% vfold_cv(v=5) #creating 5 folds cross validation 
genre_cv
```

```{r}
doParallel::registerDoParallel() #build trees in parallel
#200s

tree_rs <- tune_grid(
  tree_spec_tune, #model specification 
  genre ~ ., #function 
  resamples = genre_cv, #resamples to use
  grid = tree_grid, #grid to try
  metrics = metric_set(accuracy) #how to assess which combinations are best 
)

tree_rs
```

Use autoplot() to examine how different parameter configurations relate to accuracy 
```{r}
autoplot(tree_rs) + theme_light()
```

```{r select_hyperparam}
show_best(tree_rs)
select_best(tree_rs)
```


We can finalize the model specification where we have replaced the tune functions with optimized values.

```{r final_tree_spec}
final_tree <- finalize_model(tree_spec_tune, select_best(tree_rs))
```

This model has not been fit yet though.

```{r final_tree_fit}
final_tree_fit <- last_fit(final_tree, genre ~., genre_split)

final_tree_fit$.predictions
```

#Visualize variable importance
```{r tree_vip}
final_tree_fit 
```


#initial split and pre-processing recipe. 

Then you can run all the models on the intiial data and split. 

Tune the models and gather the final performance of the models. 

