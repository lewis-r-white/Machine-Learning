---
title: "Lab 3 Demo"
author: "Lewis White"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(skimr)
```

## Data Wrangling and Exploration
```{r data}
#load and inspect the data
ames <- AmesHousing::make_ames()
```

##Train a model
```{r intial_split}
# Stratified sampling with the rsample package
set.seed(123) #set a seed for reproducibility
split <- initial_split(data = ames,
                       prop = .70,
                       strata = "Sale_Price")

split #tells you number of obs in training, testing, and total. Need to actually assign groups 

ames_train <- training(split) #creating the training data based on the split

ames_test <- testing(split) #creating the test data based on the split
  
ames_train
skim(ames_train)
```


Dummy encoding 

Food:
1. Lasagna
2. Pizza
3. Meatloaf
4. Pizza 

Lasagna  Pizza  Meatloaf
1           0       0    
0           1       0
0           0       1
0           1       0

if both encoding and standardizing, want to make sure you encode and then standarize 


```{r model_data}
#Create training feature matrices using model.matrix() (auto encoding of categorical variables)
X <- model.matrix(Sale_Price ~ ., ames_train)[,-1] # Sale_Price ~ . just means compare Sale to all of the predictors in the model. Make sure to remove the intercept column!!

head(X)
  
# transform y with log() transformation
Y <- log(ames_train$Sale_Price)

head(Y)
```

```{r glmnet}
#fit a ridge model, passing X,Y,alpha to glmnet()
library(glmnet)

ridge <- glmnet(x = X, 
                y = Y,
                alpha = 0)


#plot() the glmnet model object
  
plot(ridge, xvar = "lambda")  
```

```{r}
# lambdas applied to penalty parameter.  Examine the first few

ridge$lambda %>%   #100 lambda values
  head() 


# small lambda results in large coefficients
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 100]

# what about for small coefficients?
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 1]
```


How much improvement to our loss function as lambda changes?

##Tuning
```{r cv.glmnet}
# Apply CV ridge regression to Ames data.  Same arguments as before to glmnet()

#RIDGE
ridge <- cv.glmnet(x = X, 
          y = Y,
          alpha = 0)  #really similar to before, but this time we built a bunch of different versions of the model 



# Apply CV lasso regression to Ames data

lasso <- cv.glmnet(x = X, 
                   y = Y, 
                   alpha = 1) 

# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")  

plot(lasso, main = "Lasso penalty\n\n")  #shows the number of features at the top. lasso performs feature selection, so we can see the number decreasing. As we remove features at the beginning, the MSE decreases. 

#first dashed line in the minimum MSE. second dashed line is 1 standard error away from the minimum value. That is the model with the fewest features within 1 standard error of the optimal value. With the absence of certainty of optimal lambda, often take the 1 stand error option because simpler models better / more interpretable. 

#initial decrease of MSE as lambda increases indicates that initial model had some overfitting 
```

10-fold CV MSE for a ridge and lasso model. What's the "rule of 1 standard deviation"?

In both models we see a slight improvement in the MSE as our penalty log(Î») gets larger, suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to increase. 

Let's examine the important parameter values apparent in the plots.
```{r}
# Ridge model


# minimum MSE


# lambda for this min MSE


# lambda for this MSE


# Lasso model
min(lasso$cvm)       # minimum MSE
lasso$lambda.min     # lambda for this min MSE


# 1-SE rule
lasso$lambda.1se  # lambda for this MSE


# No. of coef | 1-SE MSE
```

```{r}
# Ridge model
ridge_min <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)
# Lasso model
lasso_min <- glmnet(
  x = X,
  y = Y,
  alpha = 1
)
par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")
# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```


```{r }
skim(ames_train)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.